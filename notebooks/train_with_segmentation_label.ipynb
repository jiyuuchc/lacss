{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kks5J-TD8-Bh"
   },
   "source": [
    "# LACSS Supervised Training Demo\n",
    "\n",
    "This notebook shows the general workflow of supervised training an LACSS model from scratch. \n",
    "\n",
    "This data uses a small dataset from the [Cell Image Library](http://www.cellimagelibrary.org/home) collection.\n",
    "\n",
    "We will go through these steps:\n",
    "\n",
    "- Setup the data pipeline\n",
    "- Initialize a model trainer\n",
    "- Perform model training\n",
    "- Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp1Y6zHl9ddY"
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ivh9LzC89QK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jiyuuchc/lacss@lacss1\n",
    "\n",
    "from os.path import join\n",
    "import imageio.v2 as imageio\n",
    "import json\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.color import label2rgb\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "\n",
    "import lacss.data\n",
    "from lacss.train import LacssTrainer, VMapped, TFDatasetAdapter\n",
    "from lacss.ops import patches_to_label\n",
    "from lacss.utils import show_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr0QliBABDOh"
   },
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqdox1oOccv4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First download the dataset\n",
    "\n",
    "!wget -c https://data.mendeley.com/public-files/datasets/894mmsd9nj/files/568e524f-9a95-45a6-9f80-3619969c2a37/file_downloaded -O images.zip\n",
    "\n",
    "import zipfile\n",
    "\n",
    "data_path = Path('image_data')\n",
    "with zipfile.ZipFile('images.zip', \"r\") as f:\n",
    "    f.extractall(data_path)\n",
    "\n",
    "img = imageio.imread(data_path / 'train' / '000_img.png')\n",
    "gt = imageio.imread(data_path / 'train'/ '000_masks.png')\n",
    "\n",
    "show_images([\n",
    "    img,\n",
    "    gt,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGjwDe_GhRNu"
   },
   "source": [
    "Lacss expect training data from a python generator that produces the following data:\n",
    "\n",
    "```\n",
    "x_data, y_data = (\n",
    "  {\n",
    "    \"image\": ndarray[B, W, H, C],\n",
    "    \"gt_locations\": ndarray[B, N, 2]\n",
    "  },\n",
    "  {\n",
    "    \"gt_labels\": ndarray[B, W, H]\n",
    "  }\n",
    ")\n",
    "```\n",
    "\n",
    "Here we will set up the data pipeline using tensorflow.dataset library, which has many useful utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3bSK8QDEKlM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "def parser(data):\n",
    "    image = data['image']\n",
    "    label = data['label']\n",
    "    locations = data['centroids']\n",
    "\n",
    "    height = tf.shape(image)[0]\n",
    "    width = tf.shape(image)[1]\n",
    "\n",
    "    # simple augmentations\n",
    "    if tf.random.uniform(()) >= 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        label = label[:, ::-1]\n",
    "        locations = locations * [1, -1] + [0, width]\n",
    "\n",
    "    if tf.random.uniform(()) >= 0.5:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        label = label[::-1, :]\n",
    "        locations = locations * [-1, 1] + [height, 0]\n",
    "\n",
    "    # It is important to pad the locations tensor so that all elements of the dataset are of the same shape\n",
    "    n_pad = 512 - len(locations)\n",
    "    locations = tf.pad(locations, [[0, n_pad], [0,0]], constant_values=-1)\n",
    "\n",
    "    return (\n",
    "        dict(\n",
    "            image = image,\n",
    "            gt_locations = locations, \n",
    "        ),\n",
    "        dict(\n",
    "            gt_labels = label,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "batch_size = 1\n",
    "imgfiles = [join(data_path, 'train', f'{k:03d}_img.png') for k in range(89)]\n",
    "maskfiles = [join(data_path, 'train', f'{k:03d}_masks.png') for k in range(89)]\n",
    "\n",
    "# create a tensowflow dataset from the files on disk\n",
    "ds = (\n",
    "    lacss.data.dataset_from_img_mask_pairs(imgfiles, maskfiles)\n",
    "    .map(parser)\n",
    "    .repeat()\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "# make sure the dataset has the correct element structure\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GESuO6zM9tso"
   },
   "source": [
    "## Initialize a trainer\n",
    "\n",
    "The ```lacss.train.LacssTrainer``` class is the main interface we use for training. It needs a few things to start:\n",
    "\n",
    "- A configuration dictionary to override the default model hyperparameters.\n",
    "- An optional random seed value to control the process of stochastic grandient descent\n",
    "- An optional strategy specify the training backend to use. Here we used VMapped which is suitable for single GPU training on batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2mp9sJM-Tul",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model configuration. We override a few default hyperparameters\n",
    "cfg = {\n",
    "  \"backbone\": {\n",
    "    \"drop_path_rate\": 0.4\n",
    "  },\n",
    "  \"segmentor\": {\n",
    "    \"instance_crop_size\": 128\n",
    "  }\n",
    "}\n",
    "\n",
    "# LacssTrainer is the main class for model training\n",
    "trainer = LacssTrainer(\n",
    "    cfg,\n",
    "    seed=1234, # RNG seed\n",
    "    strategy=VMapped,\n",
    ")\n",
    "\n",
    "#current model hyper-parameters\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"---Current model configuration---\")\n",
    "pprint(\n",
    "    asdict(trainer.model.principal), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5qcbxs5aomk"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09jrmOyjaoAs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_steps = 15000\n",
    "validation_interval = 3000\n",
    "\n",
    "trainer.do_training(\n",
    "  TFDatasetAdapter(ds),\n",
    "  n_steps = n_steps,\n",
    "  validation_interval = validation_interval,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_tBseBbc-mw"
   },
   "source": [
    "## Visualize the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qOX43ZnYyX-"
   },
   "outputs": [],
   "source": [
    "image = imageio.imread(data_path/'test'/'000_img.png')\n",
    "gt = imageio.imread(data_path/'test'/'000_masks.png')\n",
    "\n",
    "# prediction\n",
    "model_output = trainer.model.apply(\n",
    "    dict(params = trainer.parameters),\n",
    "    image = image / 255,\n",
    ")\n",
    "pred = patches_to_label(\n",
    "    model_output, \n",
    "    input_size=image.shape[:2]\n",
    ")\n",
    "pred = np.asarray(pred)\n",
    "\n",
    "show_images([\n",
    "    image,\n",
    "    label2rgb(pred, bg_label=0),\n",
    "    label2rgb(gt, bg_label=0),\n",
    "])\n",
    "titles = ['Input', \"Prediction\", \"Ground Truth\"]\n",
    "[ax.set_title(title) for ax, title in zip(plt.gcf().get_axes(), titles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's more?\n",
    "\n",
    "- You can train for more steps\n",
    "- You can perform quantitative evaluation\n",
    "- You can incorporate validation and checkpointing into the training loop\n",
    "- You can export the trained model\n",
    "\n",
    "Check the [documentation](https://jiyuuchc.github.io/lacss/api/deploy/) for details."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
